<!DOCTYPE html>
<html>

<head>
    <link rel="stylesheet" type="text/css" href="styles.css">
    <title>Using Latent Variable Models to Predict Mouse Behavior</title>

    <script type="text/javascript" src="https://code.jquery.com/jquery-latest.min.js"></script>
    <script>
    $(function(){
    $("#3dtraj1").load("./output/mouse1/latent_trajs2_vlgp.html");
    $("#3dtraj2").load("./output/mouse1/latent_trajs2_gpfa.html");
    $("#3dtraj3").load("./output/mouse1/latent_trajs3_vlgp.html"); 
    });
    </script> 

</head>



<body>

    <header class="page-header" role="banner">
        <h1 class="project-name">Using Latent Variable Models to Predict Mouse Behavior</h1>
        <!-- <h2 class="project-tagline"></h2> -->
        
        <a href="https://github.com/styyxofficial/DSC180B-Quarter-2-Project" class="btn">View on GitHub</a>
    </header>

    <h2>Introduction</h2>

    <p> Commonly in neuroscience research, animal brains are probed with electrodes, measuring voltage signals from neurons while the animal performs a task.
        Electrodes have become so advanced nowadays that they can contain hundreds of sensors on a probe about the size of a human hair.
        During data analysis, this high dimensionality creates problems in computation and interpretation. It is hard to see what a 100-dimensional object looks like.
        Because of this, there has been a push towards dimensionality reduction models in neuroscience research. Researchers can reduce the 100 neurons down to 2 dimensions, allowing them to be visualized on a graph.
        Given that the data is recorded over time, the researchers can observe how the neurons "move" in this 2-D space. When these variables are plotted, they can be interpreted as neural trajectories. 
    </p>

    <p> The premise behind these models is that although the brain may be complex, maybe the underlying phenomenon is simple.
        For example, when you walk there are many millions of neurons firing, yet walking is simple, and so maybe the behavior of the millions neurons can be explained with just a few variables.
        For terminology, dimensionality reduced variables will be called latent variables, because they are not observed, but explain the observed data.
    </p>

    <p> For our project, we use Latent Variable Models to extract neural trajectories from a mouse as it performs a decision-making task.
    </p>

    <br>
    <h2>What are we trying to predict?</h2>

    <p> We are trying to predict whether a mouse will turn a wheel clockwise (Wheel Turned Right) or counter-clockwise (Wheel Turned Left).
        By plotting our neural trajectories, we may observe that there is a difference in the brain's "path" when the mouse turns a wheel clockwise vs counter-clockwise.
        We will use this difference to create a classification algorithm to predict which direction the mouse will choose.
        Figures 1 and 2 show example trajectories that we might obtain.
    </p>

    <figure>
        <img src="output/imgs/example_trajectory1.png" alt="Example Trajectory 1" style="width:566px;height:454px;"  class="center">
        <figcaption>Fig.1 - Example trajectories that would be differentiable between behaviors. The red dashed line indcates separability between the trajectories. We can clearly see that the 2 behaviors follow different paths.</figcaption>
    </figure>
    <br>
    <figure>
        <img src="output/imgs/example_trajectory2.png" alt="Example Trajectory 2" style="width:566px;height:454px;"  class="center">
        <figcaption>Fig.2 - Example trajectories that would not be differentiable between behaviors. As you can see, it would not be possible to say that the behaviors followed different trajectories.</figcaption>
    </figure>

    <br>
    <h2>What is our Data?</h2>
    <p> We use data collected by the International Brain Laboratory [IBL], which inserted Neuropixel probes into mice brains as they performed a decision making task.
        The task was as follows:

        <ol type="1">
            <li>A vertical grating is shown on either the mouse's left or right visual field. Simultaneously, a 5 kHz sine wave is played for 100 ms. These are the stimuli and go-cue respectively.</li>
            <li>The mouse uses a lego wheel to move the vertical grating to the center of the screen.</li>
            <li>If correctly performed, the mouse receives a water reward. If incorrect, a noise burst is played.</li>
        </ol>
    </p>

    <figure>
        <img src="output/imgs/exp_setup1.png" alt="Experimental Setup" style="width:700px;height:380px;"  class="center">
        <figcaption>Fig.3 - Experimental Setup showing the vertical grating and the decision of the mouse moving the wheel.</figcaption>
    </figure>

    <p>
        The mouse repeated this task hundreds of times with probes inserted in its brain recording voltage signals at 384 locations.
        IBL has taken this raw electrophyisology data and performed their own preprocessing to give us neuronal units (clusters) and their spike times.
    </p>

    <figure>
        <img src="output/imgs/mouse_raster.png" alt="Raster Plot 1" style="width:953px;height:534px;"  class="center">
        <figcaption>Fig.4 - Raster plot of neurons over a session. Each black dot is a neuron firing (spike). Bars on the right indicate brain regions.</figcaption>
    </figure>

    <br>
    <h2>What models are we using?</h2>

    <h3>Gaussian Process Factor Analysis</h3>
   
    <p>
        Gaussian Process Factor Analysis (GPFA) models the observed data, y, as a linear combination of low-dimensional latent variables, z, using a Gaussian process. 
        The latent variables are modeled as a Gaussian process with mean, m, and covariance function, k_z(z_i, z_j). The observed data is modeled as a linear 
        combination of the latent variables, given by y = Cz + ε, where C is a loading matrix and ε is a noise term.
    </p>
    
    <p>
        The goal of GPFA is to infer the latent variables and the loading matrix given the observed data. This can be done using maximum likelihood estimation by 
        maximizing the log-likelihood of the data, given by:
    </p>
    
    <figure>
        <img src="output/imgs/LatxFormula.PNG" alt="Latex Formula" style="width:484px;height:51px;"  class="center">
    </figure>
        
    <p>
        where K_y = CC^T + σ^2I is the covariance matrix of the observed data and σ^2 is the noise variance. The solution provides estimates of the latent variables 
        and the loading matrix, which can be used to reconstruct the underlying patterns in the data.
    </p>
    
    <figure>
        <img src="output/imgs/ModelData.PNG" alt="Trajectories from Model Data" style="width:878px;height:334px;"  class="center">
        <figcaption>Fig.5 - Model's attempt to extract latent dynamics from created dataset</figcaption>
    </figure>
    
    <h3>Variational Latent Gaussian Process</h3>
    
    <p>
        Variational Latent Gaussian Process (VLGP) extends the standard Gaussian Process (GP) by introducing a latent variable model to capture the underlying 
        dynamics of the time series. The latent variables z are modeled as a Gaussian Process with mean function m(z_t) and covariance function k(z_t, z_t'). 
    </p>
    
    <p>
        VLGP uses variational inference to learn the latent trajectories and basis functions by optimizing the Evidence Lower Bound (ELBO), given by:
    </p>
    
    <figure>
        <img src="output/imgs/LatexFormulaVLGP.PNG" alt="Latex Formula VLGP" style="width:441px;height:37px;"  class="center">
    </figure>
    
    <p>
        where q(z_{1:T}) is an approximate posterior distribution over the latent variables, and p(z_{1:T}) is the prior distribution over the latent variables 
        defined by the GP. The first term of the ELBO encourages the model to generate data that is consistent with the observed data, while the second term 
        encourages the approximate posterior distribution to be close to the prior distribution. Using the posterior distribution, we can capture the underlying dynamics
        of our neural data.
    </p>
    
    <figure>
        <img src="output/imgs/VLGPGraphic.png" alt="VLGP Graphic" style="width:752px;height:210px;"  class="center">
        <figcaption>Fig.6 - Visual Diagram of VLGP</figcaption>
    </figure>

    <br>
    <h2>Procedure</h2>

    <h3>EDA</h3>

    <figure>
        <img src="output/imgs/single_trial.png" alt="Model Trajectories" style="width:860px;height:480px;"  class="center">
        <figcaption>Fig.7 - Raster plot of a single trial. Y-axis represents depth.</figcaption>
    </figure>

    <p>
        From Figure 7, we can see a flurry of neural activity in the MOp as the mouse begins to move. This increase in firing rate is what we hope to capture in our model.
    </p>

    <h3>Data Cleaning</h3>
    <p>
        We chose to analyze data that came from the Primary Motor Cortex (MOp).
        We believed that the trajectories obtained from this region would be more differentiable between the different behaviors we expect the mouse to engage in.
    </p>

    <p>
        Furthermore, we filtered clusters based on their quality. IBL has a metric for determining how "good" a cluster is, which indicates their confidence that the spikes came from a single unit.
        A bad cluster would indicate that IBL thinks the spike could have come from multiple units, which could add noise to our data.
    </p>

    <h3>Model Fitting and Trajectories</h3>
    <p>
        We train the model on a window that begins 0.1 seconds before the mouse starts moving to 1 second after the mouse started moving.
        This should allow us to see how the brain changes throughout the mouse's actions.
    </p>

    <p>
        Shown below are trajectories we obtained using a variety of methods.
    </p>

    <figure>
        <div id="3dtraj1"></div>
        <figcaption>Fig.8 - Neural Trajectories when the wheel was correctly turned counter-clockwise (Red) and clockwise (Blue). Each line represents a different trial.
            This plot shows 2 latent variables computed using vLGP over time.
        </figcaption>
    </figure>
    <br>
    <figure>
        <div id="3dtraj2"></div>
        <figcaption>Fig.9 - Neural Trajectories when the wheel was correctly turned counter-clockwise (Red) and clockwise (Blue). Each line represents a different trial.
            This plot shows 2 latent variables computed using GPFA over time.
        </figcaption>
    </figure>
    <br>
    <figure>
        <div id="3dtraj3"></div>
        <figcaption>Fig.10 - Neural Trajectories when the wheel was correctly turned counter-clockwise (Red) and clockwise (Blue). Each line represents a different trial.
            This plot shows 3 latent variables computed using vLGP. Time is not shown.
        </figcaption>
    </figure>
    <br>
    <p>
        From Figure 8 we can see that the trajectories start off very different, but then begin to blend together as time goes on. This indicates that very specific processes are occurring in the brain early after the mouse begins moving the wheel.
        Figure 10 shows 3 latent variables, and it might be possible to say the trajectries are separable, however without a time dimension it is difficult to tell when this separability occured.
    </p>
    <br>
    
    <h3>Behavior Classification</h3>
    <p>
        As mentioned before, we want to see a clean separation between the trial types which we can observe in Figure 8 at Time=100ms. Using this information, we take a slice of the data at Time=100ms.
    </p>

    <figure>
        <img src="output/mouse1/imgs/slice1.png" alt="Slice Showing Separability" style="width:622px;height:490px;"  class="center">
        <figcaption>Fig.11 - Latent Variables 100 ms after movement onset when the mouse correctly turned the wheel clockwise (blue) and counter-clockwise (red) </figcaption>
    </figure>

    <p>
        Figure 11 shows very discernable classes which we train a Logistic Regression Classifier on.
    </p>

    <figure>
        <img src="output/mouse1/imgs/slice2.png" alt="Slice Showing Separability" style="width:622px;height:490px;"  class="center">
        <figcaption>Fig.12 - Latent Variables 100 ms after movement onset when the mouse correctly turned the wheel clockwise (blue) and incorrectly turned the wheel clockwise (red) </figcaption>
    </figure>

    Figure 12 shows very poor separation between the classes, but we train a Logistic Regression Classifier on it to see how well it can perform.

    <h2>Results</h2>
    <p>
        Due to an imbalanced test set, we use the metric Balanced Accuracy to evaluate our classifiers. The higher the balanced accuracy, the better the classifier performed.
        We evaluated the classifiers in 3 different scenarios, using no dimenstionality reduction vs 2 latent variables, using vLGP vs GPFA, and predicting between Class 0 and 1 or Class 0 and 2.
        Class 0 is when the mouse correctly moved the wheel clockwise, Class 1 is when the mouse correctly moved the wheel counter-clockwise, and Class 2 is when the mouse incorrectly moved the wheel clockwise.
        <table>
            <tr>
              <th># of Latent Variables</th>
              <th>Method of Dimensionality Reduction</th>
              <th>Classes</th>
              <th>Balanced Accuracy</th>
            </tr>
            <tr>
              <td>N/A</td>
              <td>No Dimensionality Reduction</td>
              <td>Class 0 vs Class 1</td>
              <td>94%</td>
            </tr>
            <tr>
                <td>2</td>
                <td>vLGP</td>
                <td>Class 0 vs Class 1</td>
                <td>93%</td>
            </tr>
            <tr>
                <td>2</td>
                <td>GPFA</td>
                <td>Class 0 vs Class 1</td>
                <td>91%</td>
            </tr>
            <tr>
                <td>2</td>
                <td>vLGP</td>
                <td>Class 0 vs Class 2</td>
                <td>50%</td>
            </tr>
            <tr>
                <td>2</td>
                <td>GPFA</td>
                <td>Class 0 vs Class 2</td>
                <td>50%</td>
            </tr>
        </table>
    </p>
    
    <!-- <div>
      <img src="output/imgs/ClassVLGP.png" alt="VLGPplot" style="width:40%,height:40%">
      <img src="output/imgs/ClassGPFA.png" alt="GPFAplot" style="width:40%,height:40%">
    </div>

    <div>
        <img src="output/imgs/ClassVLGPCorr.png" alt="VLGPplotCorr" style="width:40%,height:40%">
        <img src="output/imgs/ClassGPFACorr.png" alt="GPFAplotCorr" style="width:40%,height:40%">
    </div> -->

    <h2>Conclusion</h2>
    <p>
        Work in Progress
    </p>

    

</body>

</html>
